##########################
#  TIME SERIES ANALYSIS  #
##########################
library(NLP)
library(tm)
library(stringr)
library(qdapRegex)
library(tm.lexicon.GeneralInquirer)  # install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
library(tm.plugin.sentiment)  # install.packages("tm.plugin.sentiment", repos="http://R-Forge.R-project.org")
library(dtw)
library(igraph)
library(cluster)

# Parameters: days, sampling.hours, initial.date, no.users
days <- 7
sampling.hours <- 2
initial.date <- as.POSIXct('2017-01-01 00:00:00')
#no.users <- 3000

# Run a Twitter crawler searching for "#organization_name" 
# Run a Twitter crawler searching for Organization's tweets

# Data Import
setwd("C:1st_Week")  #Set current workspace
# Tweets mentioning "#organization_name" 
tweets.df <- read.csv(file = "tweets(01_01_17-07_01_17)", header = TRUE, sep = ",", stringsAsFactors = FALSE, 
                      colClasses = c("character", "logical", "numeric", "character", "POSIXct", "logical", "character", "character", 
                                     "character", "character", "character", "numeric", "logical", "logical", "character", "character"))
# Organization's tweets									 
tweets.organization.df <- read.csv(file = "organization_tweets(01_01_17-07_01_17)", header = TRUE, sep = ",", stringsAsFactors = FALSE)

CleanData <- function(tweets) {
  # Cleans twitter's data 
  # Args:
  #   tweets: A vector of raw tweets
  # Returns:
  #   A vector of "cleaned" tweets
  tweets <- str_replace(tweets, "RT @[a-z,A-Z,0-9,_]*: ", "")  # Remove retweet headers
  tweets <- str_replace(tweets, "RT:", "")
  tweets <- str_replace_all(tweets, "@[a-z,A-Z,0-9,_]*", "")  # Remove references to other screennames
  tweets <- str_replace_all(tweets, "&amp;", "")  # Remove ampersands encoding "&amp;"
  tweets <- str_replace_all(tweets, "&lt;", "")  # Remove "<" symbols encoding "&lt;"
  tweets <- str_replace_all(tweets, "&gt;", "")  # Remove ">" symbols encoding "&gt;"
  tweets <- rm_url(tweets)  # Remove URLs
  tweets <- str_replace_all(tweets, "<ed>", "")  # Remove emoticons encoding <ed><U+00A0><U+00BC>
  tweets <- str_replace_all(tweets, "<[a-z,A-Z,0-9,+]*>", "")
  tweets <- str_replace_all(tweets, '[",-,_,?,/,~,/,+,*,-,(,),:,.,#]', "")  # Remove punctuation marks
  tweets <- str_replace_all(tweets, "w/", " ")  # Remove "with" encoding "w/"
  tweets <- str_replace_all(tweets, "\\s+", " ")  # Remove unnecessary spaces
  return(tweets)
}

Normalize <- function(x, Min, Max) {
  # Scale data to [0-1]
  # Args:
  #   x: A vector to be normalised 
  #   Min: Minimum observed value
  #   Max: Maximum observed value
  # Returns:
  #   The normalised vector
  return((x - Min)/(Max - Min))
}

Analyze <- function(user.tweets.df, days, initial.date, sampling.hours) {
  # Create a time series of 3 normalised dimensions: #Tweets, #Retweets, Tweets and Retweets Sentiment 
  # Args:
  #   user.tweets.df: A data frame of a user's tweets
  #   days: An integer indicating the number of days of our data
  #   initial.date: A POSIXct value indicating the initial date of our data
  #   sampling.hours: An integer indicating the sampling ratio
  # Returns:
  #   A user's time series
  
  # Sampling parameters
  sampling.in.seconds <- sampling.hours * 60 * 60
  samples.per.Day <- 24 / sampling.hours
  sampling.hours.string <- paste(paste("+", sampling.hours, sep = ""), "hour")  #"+X hours"
  # Creating a time vector 
  alldays <- as.POSIXct(seq(initial.date, length = days * samples.per.Day, by = sampling.hours.string), tz = "UTC")  
  user.tweets.df$created <- as.POSIXct(user.tweets.df$created, tz = "UTC")
  # Vectors initialisation
  no.Tweets <- integer(days * samples.per.Day)
  no.Retweets <- integer(days * samples.per.Day)
  overall.sentiment <- numeric(days * samples.per.Day)
  # Create a vector for each dimension (no.Tweets, no.Retweets, overall.sentiment)
  for (i in 1:(days * samples.per.Day)) {  # Loop for each sample
    temp.df <- user.tweets.df[which(user.tweets.df$created >= initial.date & user.tweets.df$created < initial.date+sampling.in.seconds), ]
    if (nrow(temp.df)>0) {
      no.Tweets[i] <- nrow(temp.df[which(temp.df$isRetweet == FALSE), ])
      no.Retweets[i] <- nrow(temp.df[which(temp.df$isRetweet == TRUE), ])
      # Sentiment Analysis
      corpus <- Corpus(VectorSource(temp.df$text)) 
      pos <- sum(sapply(corpus, tm_term_score, terms_in_General_Inquirer_categories("Positiv")))
      neg <- sum(sapply(corpus, tm_term_score, terms_in_General_Inquirer_categories("Negativ")))
      pos.score <- tm_term_score(TermDocumentMatrix(corpus, control = list(removePunctuation = FALSE)), 
                                 terms_in_General_Inquirer_categories("Positiv"))
      neg.score <- tm_term_score(TermDocumentMatrix(corpus, control = list(removePunctuation = FALSE)), 
                                 terms_in_General_Inquirer_categories("Negativ")) 
      user.tm.df <- data.frame(positive = pos.score, negative = neg.score)
      user.tm.df <- transform(user.tm.df, net = positive - negative)
	  if (sum(pos.score) + sum(neg.score)!=0) {
		overall.sentiment[i] <- sum(pos.score) / (sum(pos.score) + sum(neg.score))  # Positive -> 1 // Negative -> 0
	  } else {
	    overall.sentiment[i] <- 0.5  # Neutral sentiment if pos.score = 0 & neg.score = 0
	  }
    } else {
      overall.sentiment[i] <- 0.5  # Neutral sentiment if the user didn't tweet anything
    }
    initial.date <- initial.date + sampling.in.seconds  # Just add the requistite number of seconds to the object
  }
  rm(temp.df)
  # Create user's time series
  user.tm.df <- data.frame("Date" = alldays, "no.Tweets" = no.Tweets, "no.Retweets" = no.Retweets, "Sentiment" = overall.sentiment)
  user.tm.df <- user.tm.df[2:4]  # Return only the 3 dimensions of the timeseries
  return(user.tm.df)
}

# Find all unique users' names
users <- unique(tweets.df$screenName)
no.users <- round(length(unique(tweets.df$screenName)) * 0.01)
users <- users[1:no.users]
# Data Cleaning
tweets.df$text <- CleanData(tweets.df$text)
tweets.organization.df$text <- CleanData(tweets.organization.df$text)
# Create a time series for each user
# and find min and max values of tweets and retweets dimensions
organization.tm <- Analyze(tweets.organization.df, days, initial.date, sampling.hours)
tweets.range <- c(0, max(organization.tm$no.Tweets))  # [Min, Max]
retweets.range <- c(0, max(organization.tm$no.Retweets))  # Min is always zero
users.tm.list <- list()
for (j in 1:no.users) {
  user.tweets.df <- tweets.df[which(tweets.df$screenName == users[j]), ]
  user.tm <- Analyze(user.tweets.df, days, initial.date, sampling.hours)
  users.tm.list <- c(users.tm.list, list(user.tm))
  tweets.range[2] <- max(max(user.tm$no.Tweets), tweets.range[2])	
  retweets.range[2] <- max(max(user.tm$no.Retweets), retweets.range[2])
}
# Compute the distance between each user and the Organization - Dissimilarity
distance <- c()
organization.tm$no.Tweets <- Normalize(organization.tm$no.Tweets, tweets.range[1], tweets.range[2])
organization.tm$no.Retweets <- Normalize(organization.tm$no.Retweets, retweets.range[1], retweets.range[2])
for (j in 1:no.users) {
  users.tm.list[[j]]$no.Tweets <- Normalize(users.tm.list[[j]]$no.Tweets, tweets.range[1], tweets.range[2])
  users.tm.list[[j]]$no.Retweets <- Normalize(users.tm.list[[j]]$no.Retweets, retweets.range[1], retweets.range[2])
  distance <- c(distance, dtw(organization.tm, users.tm.list[[j]], distance.only = TRUE)$distance)  # Dynamic time warping distance
}
names(users.tm.list) <- users  # Named list
# Remove users who are not "similar" with the Organization
similarity <- c(1 - Normalize(distance, min(distance), max(distance)))  # Transform dissimilarity to similarity
i <- 1 
avg.simil <- mean(similarity)  # Threshold
while  (i <= no.users){
  if (similarity[i] < avg.simil){
    similarity <- similarity[-i]
    users <- users[-i]
    no.users <- no.users - 1 
    users.tm.list[[i]] <- NULL
  }else{
    i <- i + 1 
  }
}

users.uni <- users
users.df <- data.frame(users, similarity, stringsAsFactors = FALSE)
# Complete Undirected Graph of Timeseries Similarities
userA.sim <- c()
userB.sim <- c()
weight <- c()
for (i in 1:(no.users - 1)) {
  for (j in (i + 1):no.users) {
    userA.sim <- c(userA.sim, users.uni[i])
    userB.sim <- c(userB.sim, users.uni[j])
    simil.dif <- users.df$similarity[which(users.df$users == users.uni[i])] - users.df$similarity[which(users.df$users == users.uni[j])]
    weight <- c(weight, abs(simil.dif))
  }
}
# Find Communitites of Timeseries-Graph via weighted Blondel
graph.sim.df <- data.frame(userA.sim, userB.sim, weight)
graph.sim.df <- graph.sim.df[!(graph.sim.df$weight > mean(graph.sim.df$weight)),] # Create a subgraph
graph.sim <- graph.data.frame(graph.sim.df , directed = FALSE, vertices = users.uni) 
comm.sim <- cluster_louvain(graph.sim, weights = weight)
communities(comm.sim)
